{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We need to conver text to numerical value*\n",
    "* We need a vocabulary mapping for each word(or character) to int\n",
    "* We need to setup a pytorch dataset\n",
    "* Make sure that each sentence (input) is same size (padding) and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vocabulary():\n",
    "    spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    \n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in Vocabulary.spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequency = {}\n",
    "        idx = 4\n",
    "        N = 50\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for token in self.tokenizer_eng(sentence):\n",
    "\n",
    "                frequency[token] = 1 + frequency.get(token, 0)\n",
    "\n",
    "                if frequency[token] == self.freq_threshold:\n",
    "                    self.stoi[token] = idx\n",
    "                    self.itos[idx] = token\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        token_sent = self.tokenizer_eng(text)\n",
    "\n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi['<UNK>']\n",
    "                for token in token_sent\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'this': 4, 'is': 5, 'a': 6, 'good': 7, 'place': 8, 'to': 9, 'find': 10, 'city': 11}\n",
      "[4, 5, 6, 7, 8, 9, 10, 6, 11, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "v = Vocabulary(freq_threshold=1)\n",
    "\n",
    "v.build_vocabulary([\"This is a good place to find a city\"])\n",
    "print(v.stoi)\n",
    "print(v.numericalize(\"This is a good place to find a city here!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, caption_file, transform=None, freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(caption_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # get the image and caption\n",
    "        self.images = self.df['image']\n",
    "        self.caption = self.df['caption']\n",
    "\n",
    "        # Create our own vocabulary\n",
    "        self.vocabulary = Vocabulary(freq_threshold)\n",
    "        self.vocabulary.build_vocabulary(self.caption.tolist())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get image\n",
    "        image_path = os.path.join(self.root_dir, self.images[index])\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # get caption\n",
    "        caption = self.caption[index]\n",
    "        num_caption = [self.vocabulary.stoi['<SOS>']]\n",
    "        num_caption += self.vocabulary.numericalize(caption)\n",
    "        num_caption.append(self.vocabulary.stoi['<EOS>'])\n",
    "        \n",
    "\n",
    "        return img, torch.tensor(num_caption)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        img = [item[0].unsqueeze(0) for item in batch]\n",
    "        img = torch.cat(img, 0)\n",
    "        target = [item[1] for item in batch]\n",
    "        target = pad_sequence(target, batch_first=False, padding_value=self.pad_idx)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "        root_folder,\n",
    "        annotation_file,\n",
    "        transform,\n",
    "        batch_size=32,\n",
    "        num_worker=1,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "):\n",
    "    dataset = FlickrDataset(root_dir=root_folder,\n",
    "                            caption_file=annotation_file, transform=transform)\n",
    "    pad_idx = dataset.vocabulary.stoi[\"<PAD>\"]\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_worker,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "loader = None\n",
    "if __name__ == \"__main__\":\n",
    "    loader, _ = get_loader(\"data/images\", \"data/captions.txt\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x16ca657c0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/ngkuissi/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/ngkuissi/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'FlickrDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      2\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m299\u001b[39m, \u001b[39m299\u001b[39m)),\n\u001b[1;32m      3\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[1;32m      4\u001b[0m ])\n\u001b[1;32m      6\u001b[0m loader, _ \u001b[39m=\u001b[39m get_loader(\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdata/images\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata/captions.txt\u001b[39m\u001b[39m\"\u001b[39m, transform\u001b[39m=\u001b[39mtransform\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m idx, (imgs, captions) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(loader):\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(imgs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(captions\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/site-packages/torch/utils/data/dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "loader, _ = get_loader(\n",
    "    \"data/images\", \"data/captions.txt\", transform=transform\n",
    ")\n",
    "\n",
    "for idx, (imgs, captions) in enumerate(loader):\n",
    "    print(imgs.shape)\n",
    "    print(captions.shape)\n",
    "    if idx==5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 0],\n",
       "        [4, 5, 0, 0],\n",
       "        [6, 7, 8, 9]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# Example sequences of different lengths\n",
    "sequences = [torch.tensor([1, 2, 3]),\n",
    "             torch.tensor([4, 5]),\n",
    "             torch.tensor([6, 7, 8, 9])]\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4, 6],\n",
      "        [2, 5, 7],\n",
      "        [3, 0, 8],\n",
      "        [0, 0, 9]])\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequence(sequences, batch_first=False, padding_value=0)\n",
    "print(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading inceptionnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): None\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception = models.inception_v3(weights='Inception_V3_Weights.DEFAULT', aux_logits=True)\n",
    "inception.aux_logits = False\n",
    "inception.AuxLogits = None\n",
    "inception.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionAux(\n",
       "  (conv0): BasicConv2d(\n",
       "    (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv1): BasicConv2d(\n",
       "    (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception.AuxLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 299, 299])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "image_path = \"data/images/667626_18933d713e.jpg\"  # Replace with your image path\n",
    "image = Image.open(image_path)\n",
    "transformation = transforms.transforms.Compose([transforms.transforms.Resize((299, 299)),\n",
    "                                                transforms.transforms.ToTensor()])\n",
    "\n",
    "image_tensor = transformation(image)\n",
    "#image_tensor = image_tensor.permute(1, 2, 0)\n",
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "print(image_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1042e-04, 1.6099e-04, 1.1182e-04, 1.4414e-04, 3.0983e-04, 5.5821e-04,\n",
      "        2.2292e-03, 1.2322e-04, 1.0763e-04, 2.4891e-04, 9.1726e-05, 8.4824e-05,\n",
      "        1.2684e-04, 2.0368e-04, 1.5469e-04, 1.2948e-04, 5.0540e-04, 1.5281e-04,\n",
      "        1.1814e-04, 2.2996e-04, 9.4163e-05, 2.1933e-04, 1.1465e-04, 2.4604e-04,\n",
      "        5.3684e-05, 1.4006e-04, 1.3197e-04, 4.4242e-04, 9.4141e-05, 1.4691e-04,\n",
      "        2.4868e-04, 6.2199e-04, 4.9452e-04, 1.2766e-03, 1.2204e-03, 2.2431e-04,\n",
      "        2.0432e-04, 7.6992e-04, 7.0516e-04, 2.9397e-04, 3.5319e-04, 2.9965e-04,\n",
      "        1.6164e-04, 1.3094e-04, 2.1130e-04, 2.1709e-04, 1.3393e-04, 3.8514e-04,\n",
      "        2.2806e-04, 1.3911e-04, 1.4848e-04, 4.0279e-04, 1.2871e-04, 3.0282e-04,\n",
      "        2.4473e-04, 8.2770e-05, 5.3783e-04, 2.5323e-04, 1.9517e-04, 6.2746e-05,\n",
      "        9.0976e-04, 2.9220e-04, 1.4304e-04, 1.9991e-04, 1.4704e-04, 8.4168e-05,\n",
      "        3.1709e-04, 8.1795e-04, 5.0133e-04, 3.9154e-04, 4.8728e-04, 4.4958e-04,\n",
      "        7.4184e-05, 2.0774e-04, 1.4131e-04, 5.5887e-04, 4.8645e-04, 6.0418e-04,\n",
      "        1.4324e-03, 1.6331e-04, 1.5995e-04, 2.0804e-04, 2.8240e-04, 1.3433e-04,\n",
      "        9.9737e-05, 2.8789e-04, 2.6298e-04, 2.2026e-04, 2.8624e-04, 6.3045e-05,\n",
      "        1.1912e-04, 1.0078e-04, 2.1567e-04, 2.7831e-04, 2.5966e-04, 2.9134e-04,\n",
      "        1.9118e-04, 2.7592e-04, 1.4059e-04, 1.7067e-04, 2.7480e-04, 1.0069e-04,\n",
      "        2.7468e-04, 3.3056e-04, 4.4454e-04, 8.9162e-05, 6.6564e-04, 5.8810e-04,\n",
      "        9.3927e-05, 1.5660e-04, 2.0696e-04, 7.8429e-05, 1.9137e-03, 1.0906e-03,\n",
      "        7.5904e-05, 1.8319e-04, 9.4765e-05, 2.1976e-04, 1.8089e-04, 3.6126e-04,\n",
      "        2.7592e-04, 6.1420e-05, 1.1875e-04, 1.6414e-04, 3.0261e-04, 2.1865e-03,\n",
      "        1.1018e-04, 1.5276e-04, 4.3679e-04, 6.9887e-05, 1.3018e-04, 2.5035e-04,\n",
      "        4.3816e-05, 2.6525e-04, 9.3561e-05, 1.4264e-04, 2.5337e-04, 1.8751e-04,\n",
      "        3.3173e-04, 2.4569e-04, 7.7462e-05, 2.7666e-04, 1.6264e-04, 3.4517e-04,\n",
      "        3.8776e-04, 9.0704e-05, 3.1789e-04, 1.9496e-04, 1.3036e-04, 2.9789e-04,\n",
      "        4.4561e-04, 5.3355e-03, 1.7566e-04, 1.7659e-04, 1.9030e-04, 3.6754e-04,\n",
      "        1.3418e-04, 3.5523e-04, 1.3085e-03, 6.8780e-04, 9.6357e-05, 3.2136e-04,\n",
      "        2.0906e-04, 3.2775e-03, 4.8103e-04, 1.4367e-03, 2.6002e-04, 7.6294e-05,\n",
      "        5.8594e-03, 1.0887e-04, 5.1858e-04, 2.1294e-03, 9.7368e-04, 3.0010e-04,\n",
      "        1.3394e-04, 4.1124e-04, 1.6337e-04, 1.5963e-04, 9.7513e-04, 4.3379e-04,\n",
      "        1.2940e-03, 7.2319e-05, 1.4905e-03, 9.2803e-05, 1.3978e-03, 8.0197e-04,\n",
      "        1.4913e-03, 2.1711e-03, 1.4106e-04, 3.2211e-04, 6.2610e-05, 1.0626e-03,\n",
      "        7.7599e-04, 8.7272e-04, 2.5778e-04, 1.3517e-03, 2.4821e-04, 2.8575e-04,\n",
      "        6.2587e-05, 2.4688e-04, 1.6085e-04, 7.0465e-04, 9.3059e-04, 1.2600e-04,\n",
      "        1.6925e-04, 2.5712e-04, 1.6283e-04, 1.9042e-04, 2.2689e-04, 5.1038e-04,\n",
      "        1.7292e-04, 4.5992e-04, 1.4830e-04, 6.6266e-04, 6.7044e-04, 7.0632e-05,\n",
      "        4.1409e-05, 1.8570e-04, 1.4825e-04, 1.0432e-04, 2.1639e-04, 1.1511e-04,\n",
      "        7.6544e-05, 2.5340e-04, 2.4050e-04, 3.6759e-04, 3.2335e-03, 3.8745e-04,\n",
      "        1.3433e-04, 1.0788e-04, 5.9886e-04, 3.3333e-04, 7.6033e-05, 2.9173e-04,\n",
      "        8.6163e-04, 2.0819e-04, 6.9049e-04, 2.9666e-03, 3.8873e-04, 2.1356e-04,\n",
      "        5.2276e-04, 3.2552e-04, 3.4337e-04, 1.4022e-04, 2.7948e-04, 7.3864e-04,\n",
      "        4.2143e-04, 3.6829e-05, 3.7191e-04, 2.3759e-04, 1.9175e-04, 1.6523e-04,\n",
      "        2.0647e-04, 2.6292e-04, 2.3954e-04, 2.6951e-04, 2.9532e-04, 5.1129e-04,\n",
      "        3.6781e-04, 6.5253e-04, 9.6597e-05, 2.3389e-04, 6.8001e-04, 9.8615e-04,\n",
      "        9.8618e-05, 1.4864e-04, 1.5234e-04, 2.6731e-04, 1.3863e-03, 2.2199e-04,\n",
      "        1.8434e-04, 8.3946e-05, 3.1141e-04, 1.8262e-04, 2.8482e-04, 1.0360e-04,\n",
      "        3.6561e-04, 1.4646e-04, 1.2072e-04, 2.0619e-04, 7.1279e-05, 1.3355e-04,\n",
      "        1.5883e-04, 2.0201e-04, 2.0508e-04, 2.3480e-04, 1.3574e-04, 8.2129e-05,\n",
      "        1.1740e-04, 1.0752e-04, 1.2088e-04, 1.6837e-04, 4.9941e-05, 2.1582e-04,\n",
      "        1.0633e-04, 1.2680e-04, 2.3041e-04, 1.5064e-04, 8.5121e-04, 9.0231e-04,\n",
      "        3.5957e-04, 4.4979e-04, 1.8853e-04, 3.2924e-04, 1.4210e-04, 1.0400e-03,\n",
      "        2.6333e-04, 4.0576e-04, 4.2993e-04, 5.6817e-04, 3.0036e-04, 4.7946e-04,\n",
      "        5.6867e-04, 1.1786e-04, 3.0989e-04, 5.2473e-04, 4.7069e-04, 2.2495e-04,\n",
      "        3.4387e-04, 6.9621e-04, 3.7542e-04, 5.5321e-04, 6.3652e-04, 2.2237e-04,\n",
      "        3.7189e-04, 4.4793e-04, 3.7816e-04, 6.0637e-04, 3.0006e-04, 7.3183e-05,\n",
      "        1.3161e-03, 4.9141e-04, 9.4411e-05, 1.1259e-03, 6.3963e-04, 1.1184e-04,\n",
      "        3.2646e-04, 6.4597e-04, 6.3789e-04, 1.0028e-04, 1.4898e-04, 1.1375e-03,\n",
      "        9.6172e-04, 2.8527e-04, 2.2995e-04, 6.1915e-04, 1.6630e-03, 2.5258e-04,\n",
      "        1.5560e-04, 3.3483e-04, 2.8986e-04, 1.8931e-04, 2.7970e-04, 2.8136e-04,\n",
      "        8.1668e-04, 1.2896e-04, 2.0028e-03, 1.0720e-03, 5.1043e-04, 1.5084e-03,\n",
      "        6.1539e-04, 1.1301e-04, 2.1332e-04, 2.4863e-04, 4.4581e-04, 1.0458e-03,\n",
      "        1.2505e-04, 2.4994e-03, 8.5562e-05, 2.2081e-04, 2.4041e-04, 1.0786e-04,\n",
      "        6.1154e-04, 3.0785e-04, 1.3094e-04, 1.2156e-04, 4.7993e-05, 2.6739e-04,\n",
      "        2.5445e-04, 1.2782e-04, 2.1409e-04, 2.2036e-04, 5.2785e-04, 1.2928e-04,\n",
      "        1.8794e-04, 4.5849e-04, 2.8108e-04, 4.1983e-05, 7.4629e-05, 2.4927e-05,\n",
      "        1.0740e-04, 3.6037e-05, 1.2860e-04, 2.9071e-04, 2.5189e-04, 7.3911e-05,\n",
      "        1.5842e-04, 1.3165e-04, 1.5655e-04, 3.9268e-04, 2.1537e-04, 1.8316e-04,\n",
      "        1.5886e-04, 2.6252e-04, 1.4265e-04, 1.6520e-04, 1.0372e-04, 2.3753e-04,\n",
      "        3.5765e-04, 1.1995e-04, 2.4320e-04, 1.6515e-04, 4.4113e-05, 9.6849e-04,\n",
      "        4.8105e-05, 1.0748e-04, 5.0680e-04, 7.1964e-04, 1.6600e-04, 3.4351e-03,\n",
      "        3.1854e-04, 5.6872e-05, 3.5122e-04, 1.5439e-04, 2.1597e-04, 5.8322e-04,\n",
      "        6.3435e-05, 1.4150e-04, 1.4337e-04, 7.5045e-05, 2.7435e-04, 7.7558e-04,\n",
      "        2.1632e-04, 4.7110e-04, 6.6700e-04, 1.0436e-02, 1.2436e-04, 4.4847e-04,\n",
      "        1.4109e-04, 1.5759e-04, 1.2676e-04, 1.4266e-04, 2.8181e-04, 2.3172e-04,\n",
      "        1.8465e-04, 4.6254e-01, 8.1903e-05, 1.7895e-04, 7.5643e-05, 1.0881e-04,\n",
      "        8.0910e-04, 5.7829e-05, 1.8194e-04, 3.2951e-04, 7.3652e-05, 1.6913e-04,\n",
      "        1.6641e-03, 2.7701e-04, 9.3132e-05, 1.3120e-02, 2.9874e-04, 2.3850e-04,\n",
      "        1.8762e-04, 3.9360e-04, 2.1170e-04, 4.6082e-05, 1.4893e-04, 1.1817e-04,\n",
      "        1.3772e-04, 3.6065e-04, 1.3325e-04, 2.6297e-04, 2.4044e-04, 5.4308e-05,\n",
      "        7.5817e-05, 3.2930e-04, 1.0263e-04, 1.3599e-04, 2.6830e-04, 9.6070e-05,\n",
      "        7.9923e-05, 1.0921e-04, 6.8181e-05, 3.9949e-04, 1.0404e-03, 1.6101e-04,\n",
      "        2.7384e-04, 2.7229e-04, 1.0572e-04, 4.7319e-05, 7.6840e-05, 7.6704e-04,\n",
      "        2.5839e-04, 5.2619e-04, 6.5121e-05, 1.2339e-04, 1.8519e-04, 2.5249e-04,\n",
      "        1.3096e-04, 3.9738e-04, 5.6828e-04, 1.3181e-04, 1.1872e-04, 5.3277e-05,\n",
      "        3.6893e-04, 1.5251e-04, 7.5352e-05, 1.2675e-04, 6.8635e-05, 3.4404e-05,\n",
      "        4.6538e-04, 2.7824e-04, 9.0699e-05, 1.5668e-04, 6.6179e-04, 4.3570e-04,\n",
      "        1.6684e-03, 1.5118e-04, 1.6561e-04, 8.6879e-05, 8.4682e-04, 4.5391e-05,\n",
      "        7.9893e-05, 1.6347e-04, 7.3868e-05, 1.1650e-03, 1.3321e-04, 1.4679e-04,\n",
      "        9.7211e-05, 8.3041e-04, 2.7794e-04, 1.2042e-04, 9.3197e-05, 5.8602e-05,\n",
      "        9.1269e-05, 2.9400e-04, 4.9469e-05, 9.2668e-04, 1.6646e-04, 4.2836e-05,\n",
      "        1.2848e-04, 2.7630e-04, 2.7060e-04, 2.3604e-04, 2.3763e-04, 2.8464e-04,\n",
      "        1.3866e-04, 2.7253e-04, 5.9472e-04, 3.1069e-04, 8.1309e-05, 5.6494e-04,\n",
      "        5.6358e-04, 9.1880e-05, 4.1362e-04, 3.5490e-04, 2.7772e-04, 1.3968e-04,\n",
      "        2.7508e-04, 1.5759e-04, 1.4603e-04, 1.3100e-04, 6.9955e-04, 4.7494e-05,\n",
      "        2.0716e-03, 1.6488e-04, 2.3233e-04, 1.0006e-04, 1.6621e-04, 1.9571e-04,\n",
      "        2.1518e-04, 6.7647e-05, 3.3608e-04, 1.5331e-04, 7.0574e-05, 1.6805e-04,\n",
      "        1.7914e-04, 2.2986e-04, 1.2150e-03, 4.0191e-04, 3.5972e-05, 7.3216e-05,\n",
      "        2.6036e-05, 3.9463e-04, 5.9073e-04, 1.3221e-03, 4.2639e-04, 5.1473e-04,\n",
      "        3.2457e-04, 3.7635e-04, 2.0681e-04, 5.0911e-04, 2.1430e-04, 2.4204e-04,\n",
      "        1.1175e-03, 2.4182e-04, 2.5550e-04, 2.0322e-04, 2.2146e-04, 1.7971e-04,\n",
      "        1.7931e-04, 4.7133e-04, 3.5050e-04, 8.5550e-05, 3.4737e-04, 2.0083e-04,\n",
      "        7.0731e-05, 8.7882e-05, 1.9160e-04, 7.6551e-04, 2.7295e-04, 2.0960e-04,\n",
      "        1.3039e-04, 1.2209e-04, 1.1110e-03, 1.4337e-04, 3.8098e-04, 4.7307e-04,\n",
      "        3.6006e-04, 4.7777e-04, 2.4344e-04, 2.6022e-04, 2.6084e-04, 2.3464e-04,\n",
      "        1.0905e-04, 2.8360e-04, 1.0801e-04, 3.1918e-04, 1.0353e-04, 6.4585e-04,\n",
      "        2.0278e-04, 4.5312e-03, 1.2413e-04, 4.2469e-04, 9.7793e-05, 9.8854e-05,\n",
      "        5.6932e-05, 8.6583e-05, 4.8621e-03, 5.1305e-04, 1.5028e-04, 2.5882e-04,\n",
      "        2.0202e-04, 2.6278e-04, 3.0581e-04, 1.1563e-03, 3.3876e-04, 2.8172e-04,\n",
      "        4.7852e-04, 3.6234e-04, 3.3653e-04, 1.2788e-04, 3.8295e-04, 3.1490e-05,\n",
      "        2.5924e-04, 2.0598e-03, 1.4470e-04, 2.1694e-04, 1.3478e-04, 1.9646e-03,\n",
      "        1.7074e-04, 2.3726e-04, 1.5957e-04, 1.4668e-04, 2.3398e-04, 8.6903e-05,\n",
      "        4.3176e-04, 2.8642e-04, 5.3112e-04, 4.1315e-02, 9.4139e-05, 3.9257e-04,\n",
      "        2.6058e-03, 2.4072e-04, 2.9425e-02, 1.0624e-04, 2.8851e-04, 1.3900e-03,\n",
      "        2.6251e-04, 1.4265e-03, 2.1433e-04, 1.1767e-04, 2.7180e-04, 2.2900e-04,\n",
      "        8.9394e-05, 9.6302e-05, 1.4078e-04, 8.6546e-05, 2.3972e-04, 3.6963e-04,\n",
      "        1.9980e-04, 1.0515e-03, 1.9957e-04, 7.2763e-04, 1.6694e-04, 7.0671e-05,\n",
      "        8.5789e-04, 8.7105e-04, 1.2389e-04, 5.4880e-04, 3.7085e-04, 1.6661e-03,\n",
      "        2.5434e-04, 4.9048e-05, 7.6842e-05, 7.8855e-05, 2.8619e-05, 1.9677e-04,\n",
      "        7.2334e-05, 4.1108e-04, 3.1081e-04, 1.6027e-03, 1.4544e-04, 8.7701e-05,\n",
      "        6.7934e-05, 1.4210e-04, 9.8601e-05, 1.6929e-04, 3.5385e-04, 3.5344e-04,\n",
      "        1.5611e-04, 2.4558e-03, 8.4164e-05, 2.5018e-04, 3.0622e-04, 1.3202e-04,\n",
      "        6.0153e-04, 1.1322e-04, 5.7280e-04, 3.5412e-05, 3.0290e-04, 5.2573e-04,\n",
      "        2.7625e-04, 4.3806e-04, 7.0782e-05, 3.5498e-04, 2.2735e-04, 2.8943e-04,\n",
      "        1.2529e-04, 4.5609e-04, 7.6495e-04, 1.8499e-04, 1.5007e-04, 6.5231e-05,\n",
      "        2.5901e-04, 3.3315e-04, 1.4680e-04, 6.7348e-05, 2.4176e-04, 3.6945e-04,\n",
      "        2.0827e-03, 2.2307e-04, 3.6565e-04, 8.9754e-05, 3.9127e-04, 1.2518e-04,\n",
      "        2.0153e-04, 6.3128e-04, 7.4442e-05, 1.6507e-04, 2.3291e-04, 4.0254e-04,\n",
      "        1.1385e-04, 3.5945e-04, 8.0437e-04, 3.3447e-04, 9.6584e-05, 7.8436e-05,\n",
      "        3.0864e-04, 3.5856e-04, 1.1937e-04, 1.3462e-04, 2.0425e-04, 1.6536e-04,\n",
      "        1.5876e-03, 4.9687e-03, 1.2948e-04, 1.9001e-04, 2.1457e-05, 3.6606e-04,\n",
      "        3.1507e-04, 2.7768e-04, 2.0248e-04, 1.3950e-04, 1.8392e-04, 6.7489e-04,\n",
      "        2.2625e-04, 4.6518e-04, 6.7441e-05, 8.9019e-05, 2.1744e-05, 7.0526e-05,\n",
      "        2.9561e-04, 1.2393e-03, 9.0804e-04, 2.0356e-03, 1.6255e-04, 8.3390e-04,\n",
      "        9.2828e-05, 1.1284e-04, 1.0066e-04, 2.5750e-03, 7.6774e-04, 6.4013e-04,\n",
      "        4.5322e-05, 6.7076e-04, 4.9014e-04, 5.7742e-05, 2.5734e-04, 8.2004e-04,\n",
      "        3.0798e-04, 2.3992e-04, 1.7898e-04, 5.9967e-04, 4.9861e-04, 7.8358e-05,\n",
      "        3.8141e-04, 5.6184e-04, 1.1467e-04, 3.8411e-04, 1.2414e-04, 1.6194e-04,\n",
      "        2.4358e-04, 9.9070e-04, 1.5267e-04, 1.8428e-04, 6.1994e-05, 1.3333e-04,\n",
      "        4.2325e-04, 3.5479e-04, 6.0621e-04, 8.7539e-04, 3.0288e-04, 2.8358e-04,\n",
      "        1.6635e-04, 1.4272e-04, 1.2293e-03, 1.8813e-03, 2.4441e-03, 2.0065e-04,\n",
      "        2.5300e-04, 9.6210e-05, 1.1434e-03, 1.5584e-03, 3.7808e-04, 6.0556e-04,\n",
      "        9.9127e-05, 3.4577e-04, 6.7776e-05, 2.2402e-04, 2.6681e-04, 9.4589e-04,\n",
      "        1.1073e-04, 1.0381e-04, 2.6077e-04, 4.5370e-04, 3.2106e-04, 3.2043e-04,\n",
      "        1.3930e-04, 8.3984e-05, 1.0341e-04, 2.4644e-04, 4.3796e-04, 5.5641e-04,\n",
      "        3.2377e-04, 2.3666e-05, 3.2651e-04, 3.0806e-04, 4.0939e-04, 6.7859e-05,\n",
      "        5.9332e-05, 7.5720e-04, 5.2683e-04, 1.8263e-04, 9.7669e-05, 4.7112e-04,\n",
      "        2.9686e-02, 5.5135e-05, 9.8324e-05, 3.9635e-04, 2.6756e-04, 2.2525e-04,\n",
      "        1.2930e-04, 2.8226e-05, 3.3290e-04, 7.7571e-05, 9.4864e-05, 2.3075e-04,\n",
      "        3.1279e-04, 5.6874e-04, 1.6183e-02, 2.0922e-04, 2.5819e-04, 7.0845e-05,\n",
      "        4.1055e-04, 6.2574e-04, 9.8933e-04, 1.6015e-04, 1.3490e-04, 6.5377e-05,\n",
      "        2.2112e-04, 1.3747e-05, 1.3081e-04, 4.8217e-04, 1.2881e-04, 3.0550e-04,\n",
      "        1.0786e-04, 2.3812e-04, 1.6031e-04, 5.7408e-05, 3.5098e-03, 3.6917e-05,\n",
      "        7.7972e-05, 5.1900e-04, 1.8438e-04, 7.0134e-04, 8.3736e-04, 1.9953e-04,\n",
      "        3.9660e-04, 1.6653e-04, 1.0460e-04, 4.9331e-04, 3.2422e-04, 5.8406e-04,\n",
      "        2.0465e-04, 7.8732e-04, 1.5450e-04, 2.0063e-04, 2.6652e-04, 5.9777e-04,\n",
      "        5.1557e-05, 7.1978e-05, 8.7917e-05, 1.2519e-04, 6.1523e-05, 2.5413e-04,\n",
      "        1.5517e-04, 2.8080e-04, 1.9982e-04, 1.6284e-04, 5.1855e-05, 1.5766e-04,\n",
      "        1.2918e-04, 6.4170e-05, 3.7932e-03, 1.1070e-04, 2.9852e-04, 3.3166e-04,\n",
      "        1.2790e-03, 2.9535e-04, 1.6351e-04, 7.7761e-05, 3.2571e-04, 3.3052e-04,\n",
      "        2.6850e-04, 1.3930e-04, 1.4651e-04, 1.0390e-04, 6.3077e-04, 2.4407e-04,\n",
      "        1.9589e-04, 1.4158e-03, 3.7523e-04, 9.2621e-05, 3.4372e-04, 3.5625e-04,\n",
      "        9.2916e-04, 9.4658e-05, 8.2245e-04, 9.1369e-04, 8.4503e-04, 1.2337e-03,\n",
      "        2.7165e-03, 1.0169e-04, 4.4060e-04, 2.0037e-04, 6.8561e-04, 2.3059e-03,\n",
      "        6.4885e-04, 4.0268e-04, 4.0496e-04, 1.2275e-04, 4.3383e-03, 4.6360e-04,\n",
      "        6.7749e-04, 1.7792e-04, 1.8099e-04, 5.7196e-04, 2.6807e-04, 5.0230e-05,\n",
      "        3.3979e-05, 1.1225e-04, 1.1962e-04, 2.8870e-04, 5.3280e-05, 1.7212e-04,\n",
      "        2.2026e-04, 8.0954e-05, 2.8058e-04, 7.0120e-04])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = inception(image_tensor)\n",
    "output[0].shape\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "\n",
    "        ## loading inception model\n",
    "        self.inception = models.inception_v3(weights='Inception_V3_Weights.DEFAULT', aux_logits=True)\n",
    "        self.inception.fc = nn.Linear(in_features=self.inception.fc.in_features, out_features=embed_size)\n",
    "        self.inception.aux_logits = False\n",
    "        self.inception.AuxLogits = None\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test for Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 256])\n",
      "torch.Size([22, 32, 256])\n",
      "torch.Size([23, 32, 256])\n",
      "torch.Size([1, 32, 256])\n",
      "torch.Size([23, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "enocder = EncoderCNN(256).to(\"mps\")\n",
    "embed = nn.Embedding(23556, 256).to(\"mps\")\n",
    "lstm = nn.LSTM(256, 256).to(\"mps\")\n",
    "lstm_2 = nn.LSTM(256, 512).to(\"mps\")\n",
    "for img, caption in loader:\n",
    "    img, caption = img.to(\"mps\"), caption.to(\"mps\")\n",
    "    output = enocder(img) # feature\n",
    "\n",
    "    output = output.unsqueeze(0)\n",
    "    caption = embed(caption) #embedding\n",
    "    print(output.shape)\n",
    "    print(caption.shape)\n",
    "    print(torch.cat((output, caption), dim=0).shape)\n",
    "    output, _ = lstm(output)\n",
    "    out = torch.cat((enocder(img).unsqueeze(0), caption), dim=0)\n",
    "    out, _ = lstm_2(out)\n",
    "    print(output.shape)\n",
    "    print(out.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questoin:* Do we need start of sequence in this case? since start of sequence is the feature created from the pretrained CNN block (encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 7.,  8.,  9.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 9., 10., 11.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat((torch.Tensor([[1,2,3],[7,8,9]]).unsqueeze(0), torch.Tensor([[4,5,6],[9,10,11]]).unsqueeze(0)), dim=-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        # teacher forcing\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "        hidden, _ = self.lstm(embeddings)\n",
    "\n",
    "        return self.linear(hidden)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary(5)\n",
    "vocabulary.build_vocabulary(pd.read_csv('data/captions.txt')['caption'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m enocder \u001b[39m=\u001b[39m EncoderCNN(\u001b[39m256\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m decoder \u001b[39m=\u001b[39m DecoderRNN(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39mlen\u001b[39m(vocabulary), num_layers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m img, caption \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m      4\u001b[0m     img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "enocder = EncoderCNN(256).to(\"mps\")\n",
    "decoder = DecoderRNN(256, 256, len(vocabulary), num_layers=1).to(\"mps\")\n",
    "for img, caption in loader:\n",
    "    img = img.to(\"mps\")\n",
    "    caption = caption.to(\"mps\")\n",
    "    features = enocder(img)\n",
    "    print(features.shape)\n",
    "    print(caption.shape)\n",
    "    out = decoder(features, caption)\n",
    "    print(out.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNtoRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size=embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "    \n",
    "    def forward(self, img, captions):\n",
    "        features = self.encoder(img)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_img(self, img, vocab, max_length=100):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature = self.encoder(img).unsqueeze(0) # adding dim for batch size \n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hidden, states = self.decoder.lstm(feature, states)\n",
    "                output = self.decoder.linear(hidden.squeeze(0)) # removing the extra dimension needed in lstm\n",
    "                predicted = output.argmax(1) # highest probablities word\n",
    "                result_caption.append(predicted.item())\n",
    "                feature = self.decoder.embed(predicted).unsqueeze(0)\n",
    "                if vocab.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "            \n",
    "            return [vocab.itos[idx] for idx in result_caption] #return the final sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/ngkuissi/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/ngkuissi/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'FlickrDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m vocabulary \u001b[39m=\u001b[39m Vocabulary(\u001b[39m5\u001b[39m)\n\u001b[1;32m      5\u001b[0m vocabulary\u001b[39m.\u001b[39mbuild_vocabulary(pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata/captions.txt\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m img, caption \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mcaption_img(img[\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), vocabulary))\n\u001b[1;32m      8\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/site-packages/torch/utils/data/dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/ImageCap/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Santity Check\n",
    "model = CNNtoRNN(256, 132, 2993, 1)\n",
    "vocabulary = Vocabulary(5)\n",
    "vocabulary.build_vocabulary(pd.read_csv('data/captions.txt')['caption'].tolist())\n",
    "for img, caption in loader:\n",
    "    print(model.caption_img(img[6].unsqueeze(0), vocabulary))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader, dataset = get_loader(\n",
    "    \"data/images/\", \"data/captions.txt\", transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    devcie = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CNN = False\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(dataset.vocabulary)\n",
    "num_layers = 1\n",
    "lr= 3e-4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"runs/data\")\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocabulary.stoi[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.encoder.inception.named_parameters():\n",
    "    if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = train_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1265 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m outputs \u001b[39m=\u001b[39m model(imgs, captions[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     10\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, outputs\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), captions\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     12\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mTraining loss\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem(), global_step\u001b[39m=\u001b[39mstep)\n\u001b[1;32m     13\u001b[0m step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for idx, (imgs, captions) in tqdm(enumerate(loader), total=len(loader), leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs, captions[:-1])\n",
    "        loss = criterion(outputs.view(-1, outputs.shape[2]), captions.view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "        writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "        step += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(loader):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    devcie = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    \n",
    "loader, dataset = get_loader(\n",
    "    \"data/images/\", \"data/captions.txt\", transform\n",
    ")\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(dataset.vocabulary)\n",
    "num_layers = 1\n",
    "\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'data/images/3757598567_739b7da835.jpg'  # Replace with your image path\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS>',\n",
       " 'a',\n",
       " 'man',\n",
       " 'in',\n",
       " 'a',\n",
       " 'red',\n",
       " 'shirt',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'on',\n",
       " 'a',\n",
       " 'rock',\n",
       " '.',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.caption_img(image.unsqueeze(0), dataset.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageCap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
